{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#Turn off scientific notation in Pandas\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "\n",
    "# Import data\n",
    "bom_gross = pd.read_csv('Data/bom.movie_gross.csv.gz', compression = 'gzip')\n",
    "imbd_name = pd.read_csv('Data/imdb.name.basics.csv.gz', compression = 'gzip')\n",
    "imbd_akas = pd.read_csv('Data/imdb.title.akas.csv.gz', compression = 'gzip')\n",
    "imbd_basics = pd.read_csv('Data/imdb.title.basics.csv.gz', compression = 'gzip')\n",
    "imbd_crews = pd.read_csv('Data/imdb.title.crew.csv.gz', compression = 'gzip')\n",
    "imbd_principals = pd.read_csv('Data/imdb.title.principals.csv.gz', compression = 'gzip')\n",
    "imbd_ratings = pd.read_csv('Data/imdb.title.ratings.csv.gz', compression = 'gzip')\n",
    "rt_info = pd.read_csv('Data/rt.movie_info.tsv.gz', delimiter='\\t', compression = 'gzip')\n",
    "rt_reviews = pd.read_csv('Data/rt.reviews.tsv.gz', delimiter='\\t', compression = 'gzip', encoding='latin-1')\n",
    "tmbd = pd.read_csv('Data/tmdb.movies.csv.gz', compression = 'gzip')\n",
    "tn_budget = pd.read_csv('Data/tn.movie_budgets.csv.gz', compression = 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determined which title_ids have more than one original title listed. \n",
    "#Created a new dataset consisting of these title_ids, and dropped these title_ids from the main dataset. \n",
    "#Deleted duplicate title_ids within new dataset.\n",
    "originals = (imbd_akas.loc[imbd_akas.is_original_title == 1])\n",
    "original_repeats = list(originals.loc[originals['title_id'].duplicated() == True].title_id.unique())\n",
    "or_duplicate = imbd_akas.loc[imbd_akas.title_id.isin(original_repeats)]\n",
    "imbd_akas = imbd_akas.loc[imbd_akas.title_id.isin(original_repeats) == False]\n",
    "or_duplicate = or_duplicate.loc[or_duplicate.is_original_title == 1]\n",
    "or_duplicate = or_duplicate.loc[or_duplicate.title_id.duplicated() == False]\n",
    "\n",
    "#Determined which title_ids have an original title listed. \n",
    "#Created a new dataset consisting of these title_ids, and dropped these title_ids from the main dataset. \n",
    "#Deleted duplicate title_ids within new dataset.\n",
    "original_nrp = list(imbd_akas.loc[imbd_akas.is_original_title == 1].title_id.unique())\n",
    "or_nodup = imbd_akas.loc[imbd_akas.title_id.isin(original_nrp)]\n",
    "imbd_akas = imbd_akas.loc[imbd_akas.title_id.isin(original_nrp) == False]\n",
    "or_nodup = or_nodup.loc[or_nodup.is_original_title == 1]\n",
    "\n",
    "#Determined which title_ids have more than one title listed. Created a new dataset consisting of\n",
    "#title_ids without more than one title listed, and dropped these title_ids from the main dataset.\n",
    "non_or_rp = list(imbd_akas.loc[imbd_akas.title_id.duplicated()].title_id.unique())\n",
    "non_or_nrp = imbd_akas.loc[imbd_akas.title_id.isin(non_or_rp) == False]\n",
    "imbd_akas = imbd_akas.loc[imbd_akas.title_id.isin(non_or_rp)]\n",
    "\n",
    "#Determined which title_ids have a row with a region listed as \"US\" or the language listed as \"en\".\n",
    "#Created a new dataset consisting of these title_ids, and dropped these title_ids from the main dataset.\n",
    "#In the new dataset, deleted rows that weren't listed either as \"US\" or \"en\", and then deleted duplicates.\n",
    "us_or_en = list(imbd_akas.loc[(imbd_akas.region == \"US\") | (imbd_akas.language == \"en\")].title_id.unique())\n",
    "us_en = imbd_akas.loc[imbd_akas.title_id.isin(us_or_en)]\n",
    "imbd_akas = imbd_akas.loc[imbd_akas.title_id.isin(us_or_en) == False]\n",
    "us_en = us_en.loc[(us_en.region == \"US\") | (us_en.language == \"en\")]\n",
    "us_en = us_en.loc[us_en.title_id.duplicated() == False]\n",
    "\n",
    "#Created a new dataset that is equal to the remaining original dataset, but with duplicates removed.\n",
    "no_us_en = imbd_akas.loc[imbd_akas.title_id.duplicated() == False]\n",
    "\n",
    "#Concated the sliced datafiles\n",
    "imbd_akas_dfs = [or_duplicate, or_nodup, non_or_nrp, us_en, no_us_en]\n",
    "imbd_akas_cleaned = pd.concat(imbd_akas_dfs)\n",
    "\n",
    "#Dropped unnecessary columns\n",
    "#imbd_akas_cleaned = imbd_akas_cleaned.drop(columns = ['ordering', 'region', 'language', 'types', \n",
    "#                                                      'attributes', 'is_original_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove null values for 'studio'. Set 'foreign_gross' to float and 'year' to string\n",
    "bom_gross['studio'] = bom_gross.studio.fillna('Unknown')\n",
    "bom_gross['foreign_gross'] = pd.to_numeric(bom_gross.foreign_gross, downcast = 'float', errors = 'coerce')\n",
    "bom_gross['year'] = bom_gross['year'].astype(str)\n",
    "\n",
    "#Use median data to fill in null values for domestic and foreign gross\n",
    "bom_median = bom_gross\n",
    "bom_median['domestic_gross'] = bom_median.domestic_gross.fillna(value=bom_median.domestic_gross.median())\n",
    "bom_median['foreign_gross'] = bom_median.foreign_gross.fillna(value=bom_median.foreign_gross.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bom_median.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop \"birth_year\" and \"death_year\" columns. Fill in null values with 'null'.\n",
    "imbd_name = imbd_name.drop(columns = ['birth_year', 'death_year'])\n",
    "imbd_name = imbd_name.fillna('null')\n",
    "\n",
    "#Turn 'primary_profession' and 'known_for_titles' from string to list\n",
    "imbd_name['primary_profession'] = imbd_name['primary_profession'].str.split(',')\n",
    "imbd_name['known_for_titles'] = imbd_name['known_for_titles'].str.split(',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

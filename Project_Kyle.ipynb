{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "%matplotlib inline\n",
    "\n",
    "#Turn off scientific notation in Pandas\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "\n",
    "# Import data\n",
    "bom_gross = pd.read_csv('Data/Zipped_Data/bom.movie_gross.csv.gz', compression = 'gzip')\n",
    "imbd_name = pd.read_csv('Data/Zipped_Data/imdb.name.basics.csv.gz', compression = 'gzip')\n",
    "imbd_akas = pd.read_csv('Data/Zipped_Data/imdb.title.akas.csv.gz', compression = 'gzip')\n",
    "imbd_basics = pd.read_csv('Data/Zipped_Data/imdb.title.basics.csv.gz', compression = 'gzip')\n",
    "imbd_crews = pd.read_csv('Data/Zipped_Data/imdb.title.crew.csv.gz', compression = 'gzip')\n",
    "imbd_principals = pd.read_csv('Data/Zipped_Data/imdb.title.principals.csv.gz', compression = 'gzip')\n",
    "imbd_ratings = pd.read_csv('Data/Zipped_Data/imdb.title.ratings.csv.gz', compression = 'gzip')\n",
    "rt_info = pd.read_csv('Data/Zipped_Data/rt.movie_info.tsv.gz', delimiter='\\t', compression = 'gzip')\n",
    "rt_reviews = pd.read_csv('Data/Zipped_Data/rt.reviews.tsv.gz', delimiter='\\t', compression = 'gzip', encoding='latin-1')\n",
    "tmbd = pd.read_csv('Data/Zipped_Data/tmdb.movies.csv.gz', compression = 'gzip')\n",
    "tn_budget = pd.read_csv('Data/Zipped_Data/tn.movie_budgets.csv.gz', compression = 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determined which title_ids have more than one original title listed. \n",
    "#Created a new dataset consisting of these title_ids, and dropped these title_ids from the main dataset. \n",
    "#Deleted duplicate title_ids within new dataset.\n",
    "originals = (imbd_akas.loc[imbd_akas.is_original_title == 1])\n",
    "original_repeats = list(originals.loc[originals['title_id'].duplicated() == True].title_id.unique())\n",
    "or_duplicate = imbd_akas.loc[imbd_akas.title_id.isin(original_repeats)]\n",
    "imbd_akas = imbd_akas.loc[imbd_akas.title_id.isin(original_repeats) == False]\n",
    "or_duplicate = or_duplicate.loc[or_duplicate.is_original_title == 1]\n",
    "or_duplicate = or_duplicate.loc[or_duplicate.title_id.duplicated() == False]\n",
    "\n",
    "#Determined which title_ids have an original title listed. \n",
    "#Created a new dataset consisting of these title_ids, and dropped these title_ids from the main dataset. \n",
    "#Deleted duplicate title_ids within new dataset.\n",
    "original_nrp = list(imbd_akas.loc[imbd_akas.is_original_title == 1].title_id.unique())\n",
    "or_nodup = imbd_akas.loc[imbd_akas.title_id.isin(original_nrp)]\n",
    "imbd_akas = imbd_akas.loc[imbd_akas.title_id.isin(original_nrp) == False]\n",
    "or_nodup = or_nodup.loc[or_nodup.is_original_title == 1]\n",
    "\n",
    "#Determined which title_ids have more than one title listed. Created a new dataset consisting of\n",
    "#title_ids without more than one title listed, and dropped these title_ids from the main dataset.\n",
    "non_or_rp = list(imbd_akas.loc[imbd_akas.title_id.duplicated()].title_id.unique())\n",
    "non_or_nrp = imbd_akas.loc[imbd_akas.title_id.isin(non_or_rp) == False]\n",
    "imbd_akas = imbd_akas.loc[imbd_akas.title_id.isin(non_or_rp)]\n",
    "\n",
    "#Determined which title_ids have a row with a region listed as \"US\" or the language listed as \"en\".\n",
    "#Created a new dataset consisting of these title_ids, and dropped these title_ids from the main dataset.\n",
    "#In the new dataset, deleted rows that weren't listed either as \"US\" or \"en\", and then deleted duplicates.\n",
    "us_or_en = list(imbd_akas.loc[(imbd_akas.region == \"US\") | (imbd_akas.language == \"en\")].title_id.unique())\n",
    "us_en = imbd_akas.loc[imbd_akas.title_id.isin(us_or_en)]\n",
    "imbd_akas = imbd_akas.loc[imbd_akas.title_id.isin(us_or_en) == False]\n",
    "us_en = us_en.loc[(us_en.region == \"US\") | (us_en.language == \"en\")]\n",
    "us_en = us_en.loc[us_en.title_id.duplicated() == False]\n",
    "\n",
    "#Created a new dataset that is equal to the remaining original dataset, but with duplicates removed.\n",
    "no_us_en = imbd_akas.loc[imbd_akas.title_id.duplicated() == False]\n",
    "\n",
    "#Concated the sliced datafiles\n",
    "imbd_akas_dfs = [or_duplicate, or_nodup, non_or_nrp, us_en, no_us_en]\n",
    "imbd_akas_cleaned = pd.concat(imbd_akas_dfs)\n",
    "\n",
    "#Dropped unnecessary columns\n",
    "imbd_akas_cleaned = imbd_akas_cleaned.drop(columns = ['ordering', 'region', 'language', 'types', \n",
    "                                                      'attributes', 'is_original_title'])\n",
    "\n",
    "#Removed spaces, capitalization and punctuation from title. emove potentially extraneous words.\n",
    "imbd_akas_cleaned.title = imbd_akas_cleaned.title.str.strip()\n",
    "imbd_akas_cleaned.title = imbd_akas_cleaned.title.apply(lambda x: x.lower())\n",
    "imbd_akas_cleaned.title = imbd_akas_cleaned.title.replace(string.punctuation, \"\")\n",
    "imbd_akas_cleaned.title = imbd_akas_cleaned.title.apply(lambda x: x.translate(str.maketrans('', '', '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~')))\n",
    "imbd_akas_cleaned.title = imbd_akas_cleaned.title.replace(['the', 'and'], value='', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove null values for 'studio'. Set 'foreign_gross' to float and 'year' to string\n",
    "bom_gross['studio'] = bom_gross.studio.fillna('Unknown')\n",
    "bom_gross['foreign_gross'] = pd.to_numeric(bom_gross.foreign_gross, downcast = 'float', errors = 'coerce')\n",
    "bom_gross['year'] = bom_gross['year'].astype(str)\n",
    "\n",
    "#Remove years and right whitespace from titles. Remove potentially extraneous words.\n",
    "years = ['\\(2010\\)', \"\\(2011\\)\", \n",
    "         \"\\(2012\\)\", \"\\(2013\\)\", \"\\(2014\\)\", \n",
    "         \"\\(2015\\)\", \"\\(2016\\)\", \"\\(2017\\)\", \"\\(2018\\)\"]\n",
    "bom_gross.title = bom_gross.title.replace(years, value='', regex=True)\n",
    "bom_gross.title = bom_gross.title.str.strip()\n",
    "bom_gross.title = bom_gross.title.apply(lambda x: x.lower())\n",
    "bom_gross.title = bom_gross.title.apply(lambda x: x.translate(str.maketrans('', '', '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~')))\n",
    "bom_gross.title = bom_gross.title.replace(['the', 'and'], value='', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(imbd_akas_cleaned, bom_gross, on = \"title\", how = \"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.loc[(merged_df.title_id.isna() == True) & (merged_df.year == \"2018\")].title.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bom_gross.title = bom_gross.title.replace(['the', 'and'], value='', regex=True)\n",
    "imbd_akas_cleaned.title = imbd_akas_cleaned.title.replace(['the', 'and'], value='', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "881"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.loc[merged_df.title_id.isna() == True].title.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "880"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.loc[merged_df.title_id.isna() == True].title.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "870"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.loc[merged_df.title_id.isna() == True].title.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.loc[merged_df.title_id.isna() == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.loc[(merged_df.domestic_gross.isna() == False)\n",
    "             | (merged_df.foreign_gross.isna() == False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use median data to fill in null values for domestic and foreign gross\n",
    "#bom_gross['domestic_gross'] = bom_gross.domestic_gross.fillna(value=bom_median.domestic_gross.median())\n",
    "#bom_gross['foreign_gross'] = bom_gross.foreign_gross.fillna(value=bom_median.foreign_gross.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bom_gross.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(merged_df.loc[(merged_df.domestic_gross.isna() == True) & (merged_df.foreign_gross.isna() == True)].title.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imbd_akas_cleaned.loc[imbd_akas_cleaned.title.str.contains('legacy') == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.loc[merged_df.title_id.isna() == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop \"birth_year\" and \"death_year\" columns. Fill in null values with 'null'.\n",
    "imbd_name = imbd_name.drop(columns = ['birth_year', 'death_year'])\n",
    "imbd_name = imbd_name.fillna('null')\n",
    "\n",
    "#Turn 'primary_profession' and 'known_for_titles' from string to list\n",
    "imbd_name['primary_profession'] = imbd_name['primary_profession'].str.split(',')\n",
    "imbd_name['known_for_titles'] = imbd_name['known_for_titles'].str.split(',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
